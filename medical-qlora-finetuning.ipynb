{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Fine-Tuning for Medical Symptom Classification\n",
    "\n",
    "This notebook demonstrates step-by-step QLoRA fine-tuning of PubMedBERT for classifying patient symptoms into medical categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Install Dependencies\n",
    "!pip install -q transformers peft bitsandbytes datasets accelerate sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Define Categories and Objectives\n",
    "medical_categories = {\n",
    "    0: 'Cardiac', 1: 'Respiratory', 2: 'Neurological', 3: 'Gastrointestinal',\n",
    "    4: 'Orthopedic', 5: 'Dermatological', 6: 'Endocrine', 7: 'Urological',\n",
    "    8: 'Psychiatric', 9: 'General'\n",
    "}\n",
    "TARGET_ACCURACY = 0.85  # 85%\n",
    "MEMORY_BUDGET = '8GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Load and Prepare Data\n",
    "import pandas as pd, re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset (CSV with columns 'symptoms' and 'category')\n",
    "df = pd.read_csv('medical_symptoms_5000_cases.csv')\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    abbrev = {'sob': 'shortness of breath','cp':'chest pain','n/v':'nausea vomiting','ha':'headache','abd':'abdominal'}\n",
    "    t = text.lower()\n",
    "    for k,v in abbrev.items(): t = t.replace(k, v)\n",
    "    t = re.sub(r\"\\d+\\s*(year|yo|age)\", 'adult', t)\n",
    "    return ' '.join(t.split())\n",
    "\n",
    "df['symptoms_clean'] = df['symptoms'].apply(clean_text)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['category'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['category'], random_state=42)\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Configure QLoRA\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model_name = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True\n",
    ")\n",
    "lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=['query','key','value','dense'], lora_dropout=0.1, bias='none', task_type='SEQ_CLS')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Load Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=10, quantization_config=quant_config, device_map='auto')\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "def count_trainable(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "print('Trainable params:', count_trainable(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Tokenize Datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "def tokenize(ex): return tokenizer(ex['symptoms_clean'], truncation=True, padding='max_length', max_length=256)\n",
    "train_ds = Dataset.from_pandas(train_df).map(tokenize, batched=True)\n",
    "val_ds = Dataset.from_pandas(val_df).map(tokenize, batched=True)\n",
    "test_ds = Dataset.from_pandas(test_df).map(tokenize, batched=True)\n",
    "\n",
    "for d in (train_ds, val_ds, test_ds): d.set_format('torch', columns=['input_ids','attention_mask','category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Train with QLoRA\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    return {'accuracy':acc,'precision':prec,'recall':rec,'f1':f1}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./results', num_train_epochs=3, per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32, gradient_accumulation_steps=2, learning_rate=2e-4,\n",
    "    logging_steps=50, evaluation_strategy='steps', eval_steps=200, save_steps=200,\n",
    "    load_best_model_at_end=True, metric_for_best_model='accuracy', fp16=True\n",
    ")\n",
    "trainer = Trainer(model, args, train_ds, val_ds, compute_metrics=compute_metrics)\n",
    "print('Baseline eval:', trainer.evaluate(test_ds))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Evaluate After Fine-Tuning\n",
    "res = trainer.evaluate(test_ds)\n",
    "print(res)\n",
    "\n",
    "preds = trainer.predict(test_ds)\n",
    "y_true, y_pred = preds.label_ids, np.argmax(preds.predictions, axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=list(medical_categories.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Save Adapter and Inference\n",
    "model.save_pretrained('./adapter')\n",
    "tokenizer.save_pretrained('./adapter')\n",
    "\n",
    "from peft import PeftModel\n",
    "base = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=10, quantization_config=quant_config, device_map='auto')\n",
    "loaded = PeftModel.from_pretrained(base, './adapter')\n",
    "\n",
    "def classify(text):\n",
    "    inp = tokenizer(clean_text(text), return_tensors='pt', truncation=True, padding='max_length', max_length=256)\n",
    "    out = loaded(**inp)\n",
    "    scores = torch.softmax(out.logits, dim=-1)[0]\n",
    "    idx = torch.argmax(scores).item()\n",
    "    return medical_categories[idx], scores[idx].item()\n",
    "\n",
    "print(classify('severe chest pain radiating to arm cold sweats'))"
   ]
  }
 ],
 "metadata": {"kernelspec": {"name": "python3"}, "language_info": {"name": "python"}},
 "nbformat": 4,
 "nbformat_minor": 5
}
