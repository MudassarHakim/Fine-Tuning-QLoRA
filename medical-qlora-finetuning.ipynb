# QLoRA Fine-Tuning for Medical Symptom Classification

---

## 1. Define Objectives and Use Cases

```python
medical_categories = {
    0: "Cardiac", 1: "Respiratory", 2: "Neurological", 3: "Gastrointestinal",
    4: "Orthopedic", 5: "Dermatological", 6: "Endocrine", 7: "Urological",
    8: "Psychiatric", 9: "General"
}
# Success metrics
TARGET_ACCURACY = 0.85
MEMORY_BUDGET = "8GB"
```

---

## 2. Select Pre-Trained Model

```python
model_name = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"
```

---

## 3. Collect and Prepare Data

```python
import pandas as pd, re
from sklearn.model_selection import train_test_split

def clean_medical_text(text):
    medical_abbrev = {
        'sob': 'shortness of breath', 'cp': 'chest pain', 'n/v': 'nausea vomiting',
        'ha': 'headache', 'abd': 'abdominal'
    }
    text = text.lower()
    for abbrev, full in medical_abbrev.items():
        text = text.replace(abbrev, full)
    text = re.sub(r'\d+\s*(year|yo|age)', 'adult', text)
    return ' '.join(text.split())

df = pd.read_csv('medical_symptoms_5000_cases.csv')
df['symptoms_clean'] = df['symptoms'].apply(clean_medical_text)
train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['category'], random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['category'], random_state=42)
```

---

## 4. QLoRA Adaptation Setup

```python
!pip install -q transformers peft bitsandbytes datasets accelerate
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True
)
lora_config = LoraConfig(
    r=16, lora_alpha=32, target_modules=["query", "key", "value", "dense"],
    lora_dropout=0.1, bias="none", task_type="SEQ_CLS"
)
```

---

## 5. Load Model and Tokenizer

```python
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
base_model = AutoModelForSequenceClassification.from_pretrained(
    model_name, num_labels=10, quantization_config=quantization_config,
    device_map="auto", trust_remote_code=True
)
base_model = prepare_model_for_kbit_training(base_model)
model = get_peft_model(base_model, lora_config)
```

---

## 6. Tokenize Dataset

```python
from datasets import Dataset

def tokenize_function(examples):
    return tokenizer(
        examples['symptoms_clean'], truncation=True, padding=True, max_length=256
    )
train_dataset = Dataset.from_pandas(train_df).map(tokenize_function, batched=True)
val_dataset = Dataset.from_pandas(val_df).map(tokenize_function, batched=True)
test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)
for d in [train_dataset, val_dataset, test_dataset]:
    d.set_format('torch', columns=['input_ids', 'attention_mask', 'category'])
```

---

## 7. Training Configuration and Execution

```python
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
    return {
        'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1
    }
training_args = TrainingArguments(
    output_dir="./medical_qlora_results", num_train_epochs=3,
    per_device_train_batch_size=16, per_device_eval_batch_size=32, gradient_accumulation_steps=2,
    learning_rate=2e-4, weight_decay=0.01, logging_dir="./logs", logging_steps=50,
    evaluation_strategy="steps", eval_steps=200, save_strategy="steps", save_steps=200, load_best_model_at_end=True,
    metric_for_best_model="eval_accuracy", greater_is_better=True, warmup_steps=100, fp16=True,
    dataloader_pin_memory=False, remove_unused_columns=False,
)
trainer = Trainer(
    model=model, args=training_args,
    train_dataset=train_dataset, eval_dataset=val_dataset,
    compute_metrics=compute_metrics, tokenizer=tokenizer
)
# Baseline test
baseline_results = trainer.evaluate(test_dataset)
print("Baseline", baseline_results)
# Training
trainer.train()
```

---

## 8. Evaluate Performance

```python
final_results = trainer.evaluate(test_dataset)
predictions = trainer.predict(test_dataset)
y_pred = np.argmax(predictions.predictions, axis=1)
y_true = predictions.label_ids
from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred, target_names=list(medical_categories.values())))
```

---

## 9. Adapter Saving and Production Inference

```python
model.save_pretrained("./medical_qlora_adapter")
tokenizer.save_pretrained("./medical_qlora_adapter")
from peft import PeftModel
base_model_loaded = AutoModelForSequenceClassification.from_pretrained(
    model_name, num_labels=10, quantization_config=quantization_config, device_map="auto"
)
model_loaded = PeftModel.from_pretrained(base_model_loaded, "./medical_qlora_adapter")
def classify_medical_symptoms(symptoms_text):
    clean_text = clean_medical_text(symptoms_text)
    inputs = tokenizer(clean_text, return_tensors="pt", truncation=True, padding=True, max_length=256)
    with torch.no_grad():
        outputs = model_loaded(**inputs)
        predictions = torch.softmax(outputs.logits, dim=-1)
    predicted_id = torch.argmax(predictions, dim=-1).item()
    confidence = predictions[0][predicted_id].item()
    return {
        'category': medical_categories[predicted_id], 'confidence': confidence,
        'category_id': predicted_id
    }
# Example usage
res = classify_medical_symptoms("severe chest pain radiating to jaw, cold sweats")
print(res)
```
